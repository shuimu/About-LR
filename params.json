{"name":"About Linear Regression","tagline":"just writing","body":"关于LR（线性回归），有蛮多自己的思考的，现在一一记录下来，作为感悟。\r\n\r\n线性回归简单看来，就是最小化损失函数。损失函数可以有很多的形式，比如：Loss = sum[ (f(xi) - y)^2 for i in (1, n) ] 这样的形式是符合最大似然的形式。\r\n假设f(xi) = wx+b的形式，即线性的形式，那么 Loss = sum[ (wx+b-y)^2 for i in (1, n) ]，如果采用sgd的优化方法，对其求偏导得到：\r\n对w求偏导：Loss'w = (wx+b-y)x\r\n对b求偏导：Loss'b = (wx+b-y)\r\n那么得到更新公式：\r\nw' = w + Loss'w*alpha\r\nb' = b + Loss'b*alpha\r\n其中，alpha是学习率，公式化简即：\r\nw' = w + (wx+b-y)x*alpha\r\nb' = b + (wx+b-y)*alpha\r\n考虑多个feature的情况，就是：\r\nw1' = w1 + (wx+b-y)x1*alpha\r\nw2' = w2 + (wx+b-y)x2*alpha\r\n...\r\nwn' = wn + (wn+b-y)xn*alpha\r\n其中，b'可以看做xi=1的情况。\r\n考虑成为向量的情况，\r\nw = w + (wx+b-y)*x*alpha\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}